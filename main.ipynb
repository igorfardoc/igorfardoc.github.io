{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocabulary import *\n",
    "from DataLoader import *\n",
    "from Model import *\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk import bleu_score\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflowjs as tfjs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('files'):\n",
    "    os.makedirs('files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('files/enc_weights'):\n",
    "    os.makedirs('files/enc_weights')\n",
    "if not os.path.exists('files/dec_weights'):\n",
    "    os.makedirs('files/dec_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/config.txt', 'w') as f:\n",
    "    f.write('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "dst = []\n",
    "with open('data/train.tags.de-en.de') as f:\n",
    "    src = list(map(lambda x: x.lower(), f.read().split('\\n')))[1:]\n",
    "with open('data/train.tags.de-en.en') as f:\n",
    "    dst = list(map(lambda x: x.lower(), f.read().split('\\n')))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "src = list(map(lambda x: ' '.join(tokenizer.tokenize(x)), src))\n",
    "dst = list(map(lambda x: ' '.join(tokenizer.tokenize(x)), dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/src.txt', 'w') as f:\n",
    "    f.write('\\n'.join(src))\n",
    "with open('files/dst.txt', 'w') as f:\n",
    "    f.write('\\n'.join(dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/src.txt', 'r') as f_in, open('files/bpe_src.rules', 'w') as f_out:\n",
    "    learn_bpe(f_in, f_out, num_symbols=4000, verbose=False)\n",
    "with open('files/dst.txt', 'r') as f_in, open('files/bpe_dst.rules', 'w') as f_out:\n",
    "    learn_bpe(f_in, f_out, num_symbols=4000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/bpe_src.rules') as f:\n",
    "    bpe_src = BPE(f, separator='~@~@')\n",
    "with open('files/bpe_dst.rules') as f:\n",
    "    bpe_dst = BPE(f, separator='~@~@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list(map(lambda x: ' '.join(bpe_src.segment(x).split(' ')), src))\n",
    "dst = list(map(lambda x: ' '.join(bpe_dst.segment(x).split(' ')), dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_src = Voc(src)\n",
    "voc_dst = Voc(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dst = list(map(lambda x: (src[x], dst[x]), range(len(src))))\n",
    "loader = DataLoader(src_dst, voc_src, voc_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 128\n",
    "HID_SIZE = 512\n",
    "enc = get_encoder(voc_src, EMB_SIZE, HID_SIZE)\n",
    "dec = get_decoder_step(voc_dst, EMB_SIZE, HID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on files/enc_weights/enc36/enc.weights: Not found: files/enc_weights/enc36; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a48f70801a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'files/enc_weights/enc36/enc.weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'files/dec_weights/dec36/dec.weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     93\u001b[0m   \"\"\"\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on files/enc_weights/enc36/enc.weights: Not found: files/enc_weights/enc36; No such file or directory"
     ]
    }
   ],
   "source": [
    "enc.load_weights('files/enc_weights/enc36/enc.weights')\n",
    "dec.load_weights('files/dec_weights/dec36/dec.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_weights = enc.trainable_weights + dec.trainable_weights\n",
    "optimizer = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(x, y):\n",
    "    \"\"\"\n",
    "    Compute mean crossentropy over all time-steps, perform gradient descent\n",
    "    :param x: input tokens, tensor of int32[batch_size, input_length]\n",
    "    :param y: output tokens, tensor of int32[batch_size, output_length]\n",
    "    :returns: mean loss funtion\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(trainable_weights)\n",
    "        \n",
    "        losses = []\n",
    "        states = enc(x)\n",
    "        res = tf.zeros([x.shape[0], HID_SIZE])\n",
    "        for i in range(y.shape[1] - 1):\n",
    "            res, prob, state_probs = dec([tf.reshape(y[:, i], [-1, 1]), res, states])\n",
    "            losses.append(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y[:, i + 1], prob)))\n",
    "\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        \n",
    "    gradients = tape.gradient(mean_loss, trainable_weights)\n",
    "    optimizer.apply_gradients([(gradients[i], trainable_weights[i]) for i in range(len(gradients))])\n",
    "    return float(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_states(states, symbols=10, until_eos=False):\n",
    "    state = tf.zeros([states.shape[0], HID_SIZE])\n",
    "    tokens = tf.convert_to_tensor([voc_dst.token_to_num['_BOS_']])\n",
    "    str_res = ''\n",
    "    if until_eos:\n",
    "        while True:\n",
    "            state, next_probs, state_probs = dec([tf.reshape(tokens, [-1, 1]), state, states])\n",
    "            tokens = tf.argmax(next_probs, axis=-1)\n",
    "            if str(voc_dst.num_to_token[int(tokens[0])]) == '_EOS_':\n",
    "                break\n",
    "            str_res += str(voc_dst.num_to_token[int(tokens[0])]) + ' '\n",
    "    else:\n",
    "        for i in range(symbols):\n",
    "            state, next_probs, state_probs = dec([tf.reshape(tokens, [-1, 1]), state, states])\n",
    "            tokens = tf.argmax(next_probs, axis=-1)\n",
    "            if str(voc_dst.num_to_token[int(tokens[0])]) == '_EOS_':\n",
    "                break\n",
    "            str_res += str(voc_dst.num_to_token[int(tokens[0])]) + ' '\n",
    "    if len(str_res) > 0:\n",
    "        return str_res[:-1]\n",
    "    return str_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random(symbols=10, until_eos=False):\n",
    "    x_batch, y_batch = loader.get_train_batch(1)\n",
    "    print('SOURCE:')\n",
    "    print(voc_src.tensor_to_strings(x_batch)[0])\n",
    "    print('REFERENCE:')\n",
    "    print(voc_dst.tensor_to_strings(y_batch)[0])\n",
    "    print('PREDICTION:')\n",
    "    states = enc(x_batch)\n",
    "    print(predict_by_states(states, symbols=symbols, until_eos=until_eos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_string(string, symbols=10, until_eos=False):\n",
    "    string = ' '.join(tokenizer.tokenize(string.lower()))\n",
    "    string = bpe_src.segment(string)\n",
    "    x = voc_src.strings_to_tensor([string])\n",
    "    states = enc(x)\n",
    "    print(predict_by_states(states, symbols=symbols, until_eos=until_eos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_test(iterations=5000):\n",
    "    references = []\n",
    "    translations = []\n",
    "    for i in range(iterations):\n",
    "        x, y = loader.get_test_batch(1)\n",
    "        references.append(voc_dst.tensor_to_strings(y)[0])\n",
    "        states = enc(x)\n",
    "        translations.append(predict_by_states(states, symbols=y.shape[1] * 2))\n",
    "    return bleu_score.corpus_bleu([[ref] for ref in references], translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, iterations, interval_of_saving_losses=0, interval_of_saving_weights=0,\n",
    "          show_losses=False, show_traslation=False):\n",
    "    batch_losses = []\n",
    "    for i in range(iterations):\n",
    "        x_batch, y_batch = loader.get_train_batch(batch_size)\n",
    "        loss = train_on_batch(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    \n",
    "        if show_losses:\n",
    "            clear_output(True)\n",
    "            plt.plot(batch_losses)\n",
    "            plt.show()\n",
    "            \n",
    "        if interval_of_saving_losses != 0:\n",
    "            if i % interval_of_saving_losses == 0:\n",
    "                plt.plot(batch_losses)\n",
    "                plt.savefig('files/losses.png')\n",
    "                plt.clf()\n",
    "            \n",
    "        if i % 100 == 0 and show_traslation:\n",
    "            predict_random()\n",
    "        \n",
    "        if interval_of_saving_weights != 0:\n",
    "            if i % interval_of_saving_weights == 0:\n",
    "                with open('files/config.txt') as f:\n",
    "                    iter_now = int(f.read())\n",
    "                if not os.path.exists('files/dec_weights/' + str(iter_now)):\n",
    "                    os.makedirs('files/dec_weights/' + str(iter_now))\n",
    "                if not os.path.exists('files/enc_weights/' + str(iter_now)):\n",
    "                    os.makedirs('files/enc_weights/' + str(iter_now))\n",
    "                enc.save_weights('files/enc_weights/' + str(iter_now) + '/enc.weights')\n",
    "                dec.save_weights('files/dec_weights/' + str(iter_now) + '/dec.weights')\n",
    "                with open('files/config.txt', 'w') as f:\n",
    "                    f.write(str(iter_now + 1))\n",
    "                with open('files/bleu_metrics.txt', 'a') as f:\n",
    "                    f.write(str(iter_now) + ': ' + str(accuracy_on_test(50)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train(32, 1000000000, interval_of_saving_losses=500, interval_of_saving_weights=500, show_losses=False)\n",
    "except Exception as e:\n",
    "    open('files/exeptions.txt', 'w').write(repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igorfardoc/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/keras_h5_conversion.py:123: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "tfjs.converters.save_keras_model(enc, 'files/enc')\n",
    "tfjs.converters.save_keras_model(dec, 'files/dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('files/voc.json', 'w') as f:\n",
    "    packed_bpe_rules_src = list(map(list, sorted(bpe_src.bpe_codes.keys(), key=bpe_src.bpe_codes.get)))\n",
    "    packed_bpe_rules_dst = list(map(list, sorted(bpe_dst.bpe_codes.keys(), key=bpe_dst.bpe_codes.get)))\n",
    "    token_to_num_src = voc_src.token_to_num\n",
    "    token_to_num_dst = voc_dst.token_to_num\n",
    "    num_to_token_dst = voc_dst.num_to_token\n",
    "    json.dump([packed_bpe_rules_src, packed_bpe_rules_dst, token_to_num_src, token_to_num_dst,\n",
    "               num_to_token_dst, '~@~@', '_EOS_', '</w>', HID_SIZE], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
